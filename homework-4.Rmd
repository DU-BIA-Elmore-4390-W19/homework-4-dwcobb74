---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "David Cobb"
date: "3/9/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(tree)
theme_set(theme_bw())
```
Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of `ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more com- prehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.

## Answer 1

```{r}
set.seed(1234)
df <- tbl_df(Boston)

for (k in 1:20){
  inTraining <- createDataPartition(df$medv, p = .75, list = F)
  training <- df[inTraining, ]
  testing <- df[-inTraining, ]
  mtry <- c(3:9)
  ntree <- seq(25, 500, len = 20)
  results <- tibble(trial = rep(NA, 140),
  mtry = rep(NA, 140),
  ntree = rep(NA, 140),
  mse = rep(NA, 140)) 
  for(i in 1:7){
    cat(sprintf('Trial: %s, mtry: %s --- %s\n', k, mtry[i], Sys.time()))
    for(j in 1:20){ 
      rf_train <- randomForest(medv ~ .,
                               data = training,
                               mtry = mtry[i],
                               ntree = ntree[j])
      mse <- mean((predict(rf_train, newdata = testing) - testing$medv)^2)
      results[(i-1)*20 + j, ] <- c(k, mtry[i], ntree[j], mse)
    }
  }
  if(exists("results_total")){
  results_total <- bind_rows(results_total, results)
  }
  else(
  results_total <- results
  )
}
```

### Graph
```{r}
p<- ggplot(data = results,
           aes(x = ntree, y = mse, col = as.factor(mtry)))
p + geom_line() +
  geom_point() +
  scale_color_brewer("mtry", palette = "Dark2")
```

Describe the results obtained;
The results indicate the mtry 6 (variables) with trees around 80 and again at trees around 280 have the lowest MSE.


Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into train/test using 50\% of your data in each split. In addition to parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated test MSE. 
2. Fit a multiple regression model to the training data and report the estimated test MSE
3. Summarize your results. 

8. In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.
# (a) Split the data set into a training set and a test set.

```{r}
set.seed(9823)
df_Carseats <- tbl_df(Carseats)
inTraining <- createDataPartition(df_Carseats$Sales, p = .50, list = F)
                                  training <- df_Carseats[inTraining,]
                                  testing <- df_Carseats[-inTraining,]
```

```{r}
# Part (a):
set.seed(9823)
n <- nrow(Carseats)
p <- ncol(Carseats) - 1  # remove the column we seek to predict i.e. Sales
train <- sample(1:n, n/2)
test <- (1:n)[-train]
```

### Part (b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain? 
```{r}
rtree.carseats <- tree(Sales ~ ., data = Carseats[train, ])
summary(rtree.carseats)
```

```{r}
plot(rtree.carseats)
text(rtree.carseats,pretty = 0)
```

### MSE
```{r}
y_hat <- predict(rtree.carseats, newdata = Carseats[test, ])
test.MSE <- mean((y_hat - Carseats[test, ]$Sales)^2)
print(test.MSE)
```

The most important indicator of sales appears to be shelf location and price  The Test MSE is 4.96.

### Part (c): Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r}
cv.carseats <- cv.tree(rtree.carseats)
names(cv.carseats)

```

```{r}
print(cv.carseats)
```

### plot the tree size

```{r}
plot(cv.carseats$size, cv.carseats$dev, type = "b")  
```

### Pick the size of the tree you want to prune to: It looks like k=8 is the smallest tree with an error close to the minimum.
```{r}
prune.carseats <- prune.tree(rtree.carseats, best = 8)

plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

### Predict the MSE using this tree:
```{r}
y_hat <- predict(prune.carseats, newdata = Carseats[test, ])
prune.MSE <- mean((y_hat - Carseats[test, ]$Sales)^2)
print(prune.MSE)
```

### Part (d): Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to de- termine which variables are most important.
```{r}
carseats.bag <- randomForest(Sales ~ ., data = Carseats, mtry = p, ntree = 500, importance = TRUE, subset = train)
y_hat <- predict(carseats.bag, newdata = Carseats[test, ])
mse.bag <- mean((Carseats[test, ]$Sales - y_hat)^2)
print(mse.bag)
```

```{r}
plot(carseats.bag)
```

```{r}
ibag <- importance(carseats.bag)
print(ibag[order(ibag[, 1]), ])
```

### Part (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

```{r}
carseats.rf <- randomForest(Sales ~ ., data = Carseats, ntree = 500, mtry = p/3, importance = TRUE, subset = train)
y_hat <- predict(carseats.rf, newdata = Carseats[test, ])
mse.rf <- mean((Carseats[test, ]$Sales - y_hat)^2)
print(mse.rf)
```

```{r}
plot(carseats.rf)
```

```{r}
irf <- importance(carseats.rf)
print(irf[order(irf[, 1]), ])
```

### 1. Fit a gradient-boosted tree to the training data and report the estimated test MSE. 

```{r}
set.seed(9823)
boost.Carseats=gbm(Sales~.,data=Carseats[train,],distribution="gaussian",n.trees=2000, interaction.depth=3)
yhat.boost=predict(boost.Carseats,newdata=Carseats[-train,], n.trees=2000)
mean((yhat.boost-Carseats[test, ]$Sales)^2)
```

### How about Gradient Boosting?

```{r}
set.seed(9823)
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 5)
gbm_Carseats <- train(Sales ~.,
                    data = Carseats[train,], 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
gbm_Carseats
```

```{r}
plot(gbm_Carseats)
```
`

```{r}
imp <- varImp(gbm_Carseats)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

### What about test MSE for GB?


```{r}
test_preds <- predict(gbm_Carseats, newdata=testing)
Carseats_test_df <- testing %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(Carseats_test_df$sq_err_gbm)
```

### 2. Fit a multiple regression model to the training data and report the estimated test MSE

```{r}
lm.fit=lm(Sales~.,data=training)
summary(lm.fit)
```

## Test MSE
```{r}
test_preds <- predict(lm.fit, newdata = testing)
carseats_test_df <- testing %>%
  mutate(y_hat_rf_4 = test_preds,
         sq_err_rf_4 = (y_hat_rf_4 - Sales)^2)
mean(carseats_test_df$sq_err_rf_4)
```

```{r}
mean((testing$Sales - predict(lm.fit, newdata = testing))^2)
```

### 3. Summarize your results.

The linear model has the lowest mean squared error at 1.02.  Gradient boosting had a mean squared error of 1.51.  Ramdom forests 3.53
