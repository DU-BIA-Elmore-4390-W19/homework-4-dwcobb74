---
title: 'Homework 4: Bags, Forests, Boosts, oh my'
author: "David Cobb"
date: "3/9/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libs, message = F, warning = F, include = F}
library(tidyverse)
library(broom)
library(glmnet)
library(caret)
library(ISLR)
library(janitor)
library(stringr)
library(rpart)
library(rpart.plot)
library(partykit)
library(randomForest)
library(MASS)
library(gbm)
library(tree)
theme_set(theme_bw())
```
Problem 1

Problem 7 from Chapter 8 in the text. To be specific, please use a sequence of `ntree` from 25 to 500 in steps of 25 and `mtry` from 3 to 9 for by 1. 

In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more com- prehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.

## Answer 1
```{r}
set.seed(1)
df <- tbl_df(Boston)
inTraining <- createDataPartition(df$medv, p = .75, list = F)
                                  training <- df[inTraining,]
                                  testing <- df[-inTraining,]
                                  
set.seed(2)
mtry <- c(3:9)
ntree <- seq(25,500, len = 20)
results <- tibble(mtry = rep(NA,140),
                  ntree = rep(NA, 140),
                  mse = rep(NA, 140))
for(i in 1:7){
  for(j in 1:20){
    rf_train <- randomForest(medv ~ .,
                              data = training,
                              mtry = mtry[i],
                              ntree = ntree[j])
mse <- mean((predict(rf_train, newdata = testing) - testing$medv)^2)
results[(i-1)*20 + j,] <- c(mtry[i], ntree[j], mse)
  }
  }
```

### Graph
```{r}
p<- ggplot(data = results,
           aes(x = ntree, y = mse, col = as.factor(mtry)))
p + geom_line() +
  geom_point() +
  scale_color_brewer("mtry", palette = "Dark2")
```

```{r}
library(doMC)
registerDoMC(cores = 4)
set.seed(1982)
results <- tibble(ntree = ntree,
                  mtry_3 = 1:20,
                  mtry_4 = 1:20,
                  mtry_5 = 1:20,
                  mtry_6 = 1:20,
                  mtry_7 = 1:20,
                  mtry_8 = 1:20,
                  mtry_9 = 1:20,
                  mse = 1:20)
for(i in 1:20){
  cat(sprintf('ntree: %s --- %s\n', ntree[i],Sys.time()))
  rf_boston_cv <- train(medv ~ .,
                      data = training,
                      method = "rf",
                      ntree = ntree[i],
                      importance = T,
                      tuneGrid = data.frame(mtry = 3:9))
  results[i, 2:8] <- rf_boston_cv$results$RMSE^2
}
```


```{r}
results
```
Problem 2
### Graph
```{r}
#thing <- c(results$ntree,results$mtry_3,results$mtry_4,results$mtry_5,results$mtry_6,results$mtry_7,results$mtry_8,results$mtry_9,results$mse)
p <- ggplot(data = results,
           aes(x = ntree, y = mse, col= as.factor(mtry_3)))
p + geom_line() +
  geom_point() +
  scale_color_brewer("mtry_3", palette = "Dark2")
```

Problem 8 from Chapter 8 in the text. Set your seed with 9823 and split into train/test using 50\% of your data in each split. In addition to parts (a) - (e), do the following:

1. Fit a gradient-boosted tree to the training data and report the estimated test MSE. 
2. Fit a multiple regression model to the training data and report the estimated test MSE
3. Summarize your results. 

8. In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable.
# (a) Split the data set into a training set and a test set.

```{r}
set.seed(9823)
df_Carseats <- tbl_df(Carseats)
inTraining <- createDataPartition(df_Carseats$Sales, p = .50, list = F)
                                  training <- df_Carseats[inTraining,]
                                  testing <- df_Carseats[-inTraining,]
```

```{r}
# Part (a):
set.seed(9823)
n <- nrow(Carseats)
p <- ncol(Carseats) - 1  # remove the column we seek to predict i.e. Sales
train <- sample(1:n, n/2)
test <- (1:n)[-train]
```

### Part (b) Fit a regression tree to the training set. Plot the tree, and inter- pret the results. What test MSE do you obtain? 
```{r}
rtree.carseats <- tree(Sales ~ ., data = Carseats[train, ])
summary(rtree.carseats)
```

```{r}
plot(rtree.carseats)
text(rtree.carseats,pretty = 0)
```

### MSE
```{r}
y_hat <- predict(rtree.carseats, newdata = Carseats[test, ])
test.MSE <- mean((y_hat - Carseats[test, ]$Sales)^2)
print(test.MSE)
```

#The most important indicator of sales appears to be 6) Price < 109.5 28   85.5800 12.190 * followed by  15) Advertising > 13.5 9   15.2700 11.920 *.  The Test MSE is 4.96.

### Part (c): Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

```{r}
cv.carseats <- cv.tree(rtree.carseats)
names(cv.carseats)

```

```{r}
print(cv.carseats)
```

### plot the tree size

```{r}
plot(cv.carseats$size, cv.carseats$dev, type = "b")  
```

### Pick the size of the tree you want to prune to: It looks like k=8 is the smallest tree with an error close to the minimum.
```{r}
prune.carseats <- prune.tree(rtree.carseats, best = 8)

plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

### Predict the MSE using this tree:
```{r}
y_hat <- predict(prune.carseats, newdata = Carseats[test, ])
prune.MSE <- mean((y_hat - Carseats[test, ]$Sales)^2)
print(prune.MSE)
```

### Part (d): Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to de- termine which variables are most important.
```{r}
carseats.bag <- randomForest(Sales ~ ., data = Carseats, mtry = p, ntree = 500, importance = TRUE, subset = train)
y_hat <- predict(carseats.bag, newdata = Carseats[test, ])
mse.bag <- mean((Carseats[test, ]$Sales - y_hat)^2)
print(mse.bag)
```

```{r}
plot(carseats.bag)
```

```{r}
ibag <- importance(carseats.bag)
print(ibag[order(ibag[, 1]), ])
```

### Part (e) Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which vari- ables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

```{r}
carseats.rf <- randomForest(Sales ~ ., data = Carseats, ntree = 500, mtry = p/3, importance = TRUE, subset = train)
y_hat <- predict(carseats.rf, newdata = Carseats[test, ])
mse.rf <- mean((Carseats[test, ]$Sales - y_hat)^2)
print(mse.rf)
```

```{r}
plot(carseats.rf)
```

```{r}
irf <- importance(carseats.rf)
print(irf[order(irf[, 1]), ])
```

### 1. Fit a gradient-boosted tree to the training data and report the estimated test MSE. 

```{r}
set.seed(9823)
boost.Carseats=gbm(Sales~.,data=Carseats[train,],distribution="gaussian",n.trees=5000, interaction.depth=4)
yhat.boost=predict(boost.Carseats,newdata=Carseats[-train,], n.trees=5000)
mean((yhat.boost-Carseats[test, ]$Sales)^2)
```

### How about Gradient Boosting?

```{r}
set.seed(9823)
grid <- expand.grid(interaction.depth = c(1, 3), 
                    n.trees = seq(0, 2000, by = 100),
                    shrinkage = c(.01, 0.001),
                    n.minobsinnode = 10)
trainControl <- trainControl(method = "cv", number = 5)
gbm_Carseats <- train(Sales ~.,
                    data = Carseats[train,], 
                    distribution = "gaussian", 
                    method = "gbm",
                    trControl = trainControl, 
                    tuneGrid = grid,
                    verbose = FALSE)
gbm_Carseats
```

```{r}
plot(gbm_Carseats)
```
`

```{r}
imp <- varImp(gbm_Carseats)$importance
rn <- row.names(imp)
imp_df <- data_frame(variable = rn, 
                     importance = imp$Overall) %>%
  arrange(desc(-importance)) %>%
  mutate(variable = factor(variable, variable))
p <- ggplot(data = imp_df,
            aes(variable, importance))
p + geom_col(fill = "#6e0000") +
  coord_flip()
```

### What about test MSE for GB?


```{r}
test_preds <- predict(gbm_Carseats, newdata= testing)
Carseats_test_df <- testing %>%
  mutate(y_hat_gbm = test_preds,
         sq_err_gbm = (y_hat_gbm - Sales)^2)
mean(Carseats_test_df$sq_err_gbm)
```

### 2. Fit a multiple regression model to the training data and report the estimated test MSE

```{r}
lm.fit=lm(Sales~.,data=training)
summary(lm.fit)
```


```{r}
test_preds <- predict(lm.fit, newdata = testing)
carseats_test_df <- testing %>%
  mutate(y_hat_rf_4 = test_preds,
         sq_err_rf_4 = (y_hat_rf_4 - Sales)^2)
mean(carseats_test_df$sq_err_rf_4)
```


```{r}
## Test MSE
mean((testing$Sales - predict(lm.fit, newdata = testing))^2)
```

### 3. Summarize your results.
